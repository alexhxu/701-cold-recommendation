{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "234693a37b994780bbbd566648d61398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4981a31d53c24ff5b8bc43555515ef9e",
              "IPY_MODEL_b186f443ac184b888f08503f1f8b1ee2",
              "IPY_MODEL_7728d3e293de4d30a4a990265b84852b"
            ],
            "layout": "IPY_MODEL_0078beba7f474736b483c17331e2a4e8"
          }
        },
        "4981a31d53c24ff5b8bc43555515ef9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171ea4af804149b2a71a213c7ec04e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_f9d3694d21af46b2bccb2f79aff3fb29",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b186f443ac184b888f08503f1f8b1ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93ac5ca6b582425d9aa83d12ffe4b600",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3baecc6a8f14438095a1f15864138cf8",
            "value": 48
          }
        },
        "7728d3e293de4d30a4a990265b84852b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ea295832634fadbee7b412f8dc9c7c",
            "placeholder": "​",
            "style": "IPY_MODEL_65fe261de3aa448eaebb23ca2cb33369",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.37kB/s]"
          }
        },
        "0078beba7f474736b483c17331e2a4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171ea4af804149b2a71a213c7ec04e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9d3694d21af46b2bccb2f79aff3fb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93ac5ca6b582425d9aa83d12ffe4b600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3baecc6a8f14438095a1f15864138cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9ea295832634fadbee7b412f8dc9c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65fe261de3aa448eaebb23ca2cb33369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a6e77ac2e314bd98d4dec7f41a3decb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_435a3a5fdd7342a2aff359bf35b6ca15",
              "IPY_MODEL_5f036b5cb53147f5af38f67feed712d6",
              "IPY_MODEL_2a9637483181448c8d66fe274c701fa5"
            ],
            "layout": "IPY_MODEL_cdeb258dd3c14c10bc72ce95aedaa046"
          }
        },
        "435a3a5fdd7342a2aff359bf35b6ca15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e0b35b120d4bd99fd2ad3711b01f41",
            "placeholder": "​",
            "style": "IPY_MODEL_7062ce5814a541b2b1781458adcba1fa",
            "value": "vocab.txt: 100%"
          }
        },
        "5f036b5cb53147f5af38f67feed712d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9fca1887c34ba399ae0df0dc8bd591",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bd9b74b248b4da096b5ed217f2f4285",
            "value": 231508
          }
        },
        "2a9637483181448c8d66fe274c701fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b14b9c6fc2f349a385f214ae0e4dc806",
            "placeholder": "​",
            "style": "IPY_MODEL_4506bdb5c53e4a8799584e4a59ea897f",
            "value": " 232k/232k [00:00&lt;00:00, 9.40MB/s]"
          }
        },
        "cdeb258dd3c14c10bc72ce95aedaa046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e0b35b120d4bd99fd2ad3711b01f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7062ce5814a541b2b1781458adcba1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc9fca1887c34ba399ae0df0dc8bd591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd9b74b248b4da096b5ed217f2f4285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b14b9c6fc2f349a385f214ae0e4dc806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4506bdb5c53e4a8799584e4a59ea897f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2972828cecd247b4b5354597f1199211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83288f3fd93d43c5a87fe36b80811719",
              "IPY_MODEL_23da903cf509470b975553033b430043",
              "IPY_MODEL_6fbcf028ab0f46ee9502f9f8d4ac90b3"
            ],
            "layout": "IPY_MODEL_de33182068c54d52a0858ee842a8f38e"
          }
        },
        "83288f3fd93d43c5a87fe36b80811719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d88bec1aabd746e4bd75c460fd74e45b",
            "placeholder": "​",
            "style": "IPY_MODEL_624bb460023d405cbe3093a5902e007e",
            "value": "tokenizer.json: 100%"
          }
        },
        "23da903cf509470b975553033b430043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d029b8b68d84f0e98b39ec1bf97b878",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b2fbb4052a24c30920ee2ca7e9efe0a",
            "value": 466062
          }
        },
        "6fbcf028ab0f46ee9502f9f8d4ac90b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de83eb4b36004249b8122da185a96225",
            "placeholder": "​",
            "style": "IPY_MODEL_5d8fa89e44604f3583bf9fcbb2c6d147",
            "value": " 466k/466k [00:00&lt;00:00, 3.69MB/s]"
          }
        },
        "de33182068c54d52a0858ee842a8f38e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88bec1aabd746e4bd75c460fd74e45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "624bb460023d405cbe3093a5902e007e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d029b8b68d84f0e98b39ec1bf97b878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2fbb4052a24c30920ee2ca7e9efe0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de83eb4b36004249b8122da185a96225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8fa89e44604f3583bf9fcbb2c6d147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34597e25bda4d8182ce27c2a6bb1087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1836254610e8402c961d9520cb6e2a1e",
              "IPY_MODEL_6d83809abe174ca8bea0c8646c29739a",
              "IPY_MODEL_1ec0203c10614d699f99ea84b32ad068"
            ],
            "layout": "IPY_MODEL_b6718c645cb14d4188291418498f7c83"
          }
        },
        "1836254610e8402c961d9520cb6e2a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c8aec506304c56940c4095792b87eb",
            "placeholder": "​",
            "style": "IPY_MODEL_2794349ba5ff4fac9ee971ae85a31453",
            "value": "config.json: 100%"
          }
        },
        "6d83809abe174ca8bea0c8646c29739a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b427e501b0749b482627ce9a39e48db",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9087d96efba84041813c4282d847250c",
            "value": 570
          }
        },
        "1ec0203c10614d699f99ea84b32ad068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4775dc57602049029006fb30bc216808",
            "placeholder": "​",
            "style": "IPY_MODEL_c4f0e508059649849a9ae935d0449a7d",
            "value": " 570/570 [00:00&lt;00:00, 56.7kB/s]"
          }
        },
        "b6718c645cb14d4188291418498f7c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c8aec506304c56940c4095792b87eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2794349ba5ff4fac9ee971ae85a31453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b427e501b0749b482627ce9a39e48db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9087d96efba84041813c4282d847250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4775dc57602049029006fb30bc216808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f0e508059649849a9ae935d0449a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51f08c3257954e53a658a5e1e5607064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abfcab30515042698d24c218b73b7a51",
              "IPY_MODEL_72e3a77f3cfe41308f462ccf8b02a7bd",
              "IPY_MODEL_1726803de0134433a6823cc76ae6f313"
            ],
            "layout": "IPY_MODEL_59801c03182b4af295ca6557615b072e"
          }
        },
        "abfcab30515042698d24c218b73b7a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151a73cb5db84957b904815b1e1179f4",
            "placeholder": "​",
            "style": "IPY_MODEL_df602c3ab7bf4de1af0b7f778e8cb025",
            "value": "model.safetensors: 100%"
          }
        },
        "72e3a77f3cfe41308f462ccf8b02a7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def79892d2064af6857b7c5fc4ae3ef3",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b27f9d5eec76410e9101628711917c65",
            "value": 440449768
          }
        },
        "1726803de0134433a6823cc76ae6f313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af24ad853fd649968e8daab9cd7142eb",
            "placeholder": "​",
            "style": "IPY_MODEL_07f7b35c097c462cb978c3b68d0dab12",
            "value": " 440M/440M [00:02&lt;00:00, 280MB/s]"
          }
        },
        "59801c03182b4af295ca6557615b072e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "151a73cb5db84957b904815b1e1179f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df602c3ab7bf4de1af0b7f778e8cb025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "def79892d2064af6857b7c5fc4ae3ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b27f9d5eec76410e9101628711917c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af24ad853fd649968e8daab9cd7142eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f7b35c097c462cb978c3b68d0dab12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Modules import and set up."
      ],
      "metadata": {
        "id": "FTqSh0054Gl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, time, json, copy, pickle, random, requests, re\n",
        "import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "GT_PGUFl4NMW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "P4VMQZHXTreH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "PK9NNM6kSOxZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the movie and ratings files."
      ],
      "metadata": {
        "id": "SUqkt-Py4Wp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_films_dict(file_path):\n",
        "    master_dict = {}\n",
        "    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
        "        for line in file:\n",
        "            full_line = line.strip()\n",
        "            id = int(full_line.split(\"::\")[0])\n",
        "            master_dict[id] = full_line\n",
        "    return master_dict\n",
        "\n",
        "def create_ratings_matrix(file_path):\n",
        "    full_array = []\n",
        "    with open(file_path, \"r\", encoding = \"latin-1\") as file:\n",
        "        for line in file:\n",
        "            full_line = line.strip()\n",
        "            full_array.append(full_line)\n",
        "        as_np_array = np.array(full_array)\n",
        "    return as_np_array"
      ],
      "metadata": {
        "id": "hqKCJ3Mc4WBT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_ratings(ratings_array):\n",
        "    \"\"\"Parse ratings array into user_ids, item_ids, ratings\"\"\"\n",
        "    user_ids = []\n",
        "    item_ids = []\n",
        "    ratings = []\n",
        "\n",
        "    for entry in ratings_array:\n",
        "        parts = entry.split(\"::\")\n",
        "        user_ids.append(int(parts[0]))\n",
        "        item_ids.append(int(parts[1]))\n",
        "        ratings.append(float(parts[2]))\n",
        "\n",
        "    return np.array(user_ids), np.array(item_ids), np.array(ratings)\n",
        "\n",
        "def prepare_item_profiles(films_dict):\n",
        "    \"\"\"Extract item profiles from films dictionary\"\"\"\n",
        "    item_profiles = {}\n",
        "    for item_id, info in films_dict.items():\n",
        "        parts = info.split(\"::\")\n",
        "        if len(parts) >= 3:\n",
        "            title = parts[1]\n",
        "            genres = parts[2]\n",
        "            # Combine title and genres as profile\n",
        "            profile = f\"{title} {genres.replace('|', ' ')}\"\n",
        "            item_profiles[item_id] = profile\n",
        "    return item_profiles"
      ],
      "metadata": {
        "id": "w5rVc4DDW5pI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "movies_path = \"movies.dat\"\n",
        "ratings_path = \"ratings.dat\"\n",
        "filmsdict = create_films_dict(movies_path)\n",
        "ratingsmat = create_ratings_matrix(ratings_path)\n",
        "# Parse ratings\n",
        "user_ids, item_ids, ratings = parse_ratings(ratingsmat)\n",
        "\n",
        "# Prepare item profiles\n",
        "item_profiles = prepare_item_profiles(filmsdict)\n",
        "\n",
        "print(f\"Loaded {len(user_ids)} ratings\")\n",
        "print(f\"Loaded {len(item_profiles)} item profiles\")"
      ],
      "metadata": {
        "id": "R9579GHa4gVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a2e7b2-33f5-4e30-dbb9-64d3555e471b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000209 ratings\n",
            "Loaded 3883 item profiles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test data split. (80:20 split according to paper.)"
      ],
      "metadata": {
        "id": "a-HbzDbHamT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data (80-20)\n",
        "indices = np.arange(len(ratings))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_users = user_ids[train_idx]\n",
        "train_items = item_ids[train_idx]\n",
        "train_ratings = ratings[train_idx]\n",
        "\n",
        "test_users = user_ids[test_idx]\n",
        "test_items = item_ids[test_idx]\n",
        "test_ratings = ratings[test_idx]\n",
        "\n",
        "print(f\"Training samples: {len(train_ratings)}\")\n",
        "print(f\"Test samples: {len(test_ratings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU4uCHX1arxL",
        "outputId": "37f2fb71-6199-4d05-924d-e636078141e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 800167\n",
            "Test samples: 200042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device as GPU if possible."
      ],
      "metadata": {
        "id": "mxH5EIVibuRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icHLbPwUbzkc",
        "outputId": "f5da1139-924c-404c-851b-9a88a26629fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://huggingface.co/google-bert/bert-base-uncased"
      ],
      "metadata": {
        "id": "RH8v4PtF9BAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "# Load BERT model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
        "                                  output_hidden_states=True)"
      ],
      "metadata": {
        "id": "7liIiE6Y7lE8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "234693a37b994780bbbd566648d61398",
            "4981a31d53c24ff5b8bc43555515ef9e",
            "b186f443ac184b888f08503f1f8b1ee2",
            "7728d3e293de4d30a4a990265b84852b",
            "0078beba7f474736b483c17331e2a4e8",
            "171ea4af804149b2a71a213c7ec04e7a",
            "f9d3694d21af46b2bccb2f79aff3fb29",
            "93ac5ca6b582425d9aa83d12ffe4b600",
            "3baecc6a8f14438095a1f15864138cf8",
            "b9ea295832634fadbee7b412f8dc9c7c",
            "65fe261de3aa448eaebb23ca2cb33369",
            "0a6e77ac2e314bd98d4dec7f41a3decb",
            "435a3a5fdd7342a2aff359bf35b6ca15",
            "5f036b5cb53147f5af38f67feed712d6",
            "2a9637483181448c8d66fe274c701fa5",
            "cdeb258dd3c14c10bc72ce95aedaa046",
            "53e0b35b120d4bd99fd2ad3711b01f41",
            "7062ce5814a541b2b1781458adcba1fa",
            "cc9fca1887c34ba399ae0df0dc8bd591",
            "1bd9b74b248b4da096b5ed217f2f4285",
            "b14b9c6fc2f349a385f214ae0e4dc806",
            "4506bdb5c53e4a8799584e4a59ea897f",
            "2972828cecd247b4b5354597f1199211",
            "83288f3fd93d43c5a87fe36b80811719",
            "23da903cf509470b975553033b430043",
            "6fbcf028ab0f46ee9502f9f8d4ac90b3",
            "de33182068c54d52a0858ee842a8f38e",
            "d88bec1aabd746e4bd75c460fd74e45b",
            "624bb460023d405cbe3093a5902e007e",
            "6d029b8b68d84f0e98b39ec1bf97b878",
            "3b2fbb4052a24c30920ee2ca7e9efe0a",
            "de83eb4b36004249b8122da185a96225",
            "5d8fa89e44604f3583bf9fcbb2c6d147",
            "a34597e25bda4d8182ce27c2a6bb1087",
            "1836254610e8402c961d9520cb6e2a1e",
            "6d83809abe174ca8bea0c8646c29739a",
            "1ec0203c10614d699f99ea84b32ad068",
            "b6718c645cb14d4188291418498f7c83",
            "33c8aec506304c56940c4095792b87eb",
            "2794349ba5ff4fac9ee971ae85a31453",
            "1b427e501b0749b482627ce9a39e48db",
            "9087d96efba84041813c4282d847250c",
            "4775dc57602049029006fb30bc216808",
            "c4f0e508059649849a9ae935d0449a7d",
            "51f08c3257954e53a658a5e1e5607064",
            "abfcab30515042698d24c218b73b7a51",
            "72e3a77f3cfe41308f462ccf8b02a7bd",
            "1726803de0134433a6823cc76ae6f313",
            "59801c03182b4af295ca6557615b072e",
            "151a73cb5db84957b904815b1e1179f4",
            "df602c3ab7bf4de1af0b7f778e8cb025",
            "def79892d2064af6857b7c5fc4ae3ef3",
            "b27f9d5eec76410e9101628711917c65",
            "af24ad853fd649968e8daab9cd7142eb",
            "07f7b35c097c462cb978c3b68d0dab12"
          ]
        },
        "outputId": "a4f3244d-7135-432d-e47e-6ff427eea454"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "234693a37b994780bbbd566648d61398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a6e77ac2e314bd98d4dec7f41a3decb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2972828cecd247b4b5354597f1199211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a34597e25bda4d8182ce27c2a6bb1087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51f08c3257954e53a658a5e1e5607064"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate vector with BERT."
      ],
      "metadata": {
        "id": "sqr2w2lF3YbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8IzNNfGb3VVq"
      },
      "outputs": [],
      "source": [
        "def get_vector_batch(texts, bert_model, bert_tokenizer, batch_size=16, device='cpu'):\n",
        "  \"\"\"Generate BERT embeddings for a batch of texts\"\"\"\n",
        "  bert_model.eval()\n",
        "  bert_model.to(device)\n",
        "  all_embeddings = []\n",
        "\n",
        "  for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating BERT embeddings\"):\n",
        "      batch = texts[i:i+batch_size]\n",
        "      encoded_input = bert_tokenizer(batch, return_tensors='pt',\n",
        "                                      padding=True, truncation=True,\n",
        "                                      max_length=512)\n",
        "      encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "\n",
        "      with torch.no_grad():\n",
        "          output = bert_model(**encoded_input)\n",
        "          hidden_states = output.hidden_states\n",
        "\n",
        "          # Sum last 4 layers\n",
        "          last4 = torch.stack(hidden_states[-4:]).sum(0)\n",
        "          # Mean pooling\n",
        "          embedding = last4.mean(dim=1)\n",
        "          all_embeddings.append(embedding)\n",
        "\n",
        "  return torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "vec = get_vector_batch([\"hello world bro i dont know what else to Say what's wrong\", \"wha tis going on\"])\n",
        "# print(vec)\n",
        "print(vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVPO0Qd4Q8CX",
        "outputId": "2a51762d-881b-46cf-b14b-5d59e5cd994a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Deep AutoEncoder."
      ],
      "metadata": {
        "id": "e0j3HJNU7xEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepAutoEncoder(nn.Module):\n",
        "  # could be modified to just take in a dims list\n",
        "  def __init__(self, input_dim=768, hidden_dim=64):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(128, hidden_dim)\n",
        "      )\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, 128),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(128, 256),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(512, input_dim),\n",
        "      )\n",
        "  def forward(self, x):\n",
        "    reduced = self.encoder(x)\n",
        "    reconstructed = self.decoder(reduced)\n",
        "    return reduced, reconstructed"
      ],
      "metadata": {
        "id": "va-ghqhC72fX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader."
      ],
      "metadata": {
        "id": "D28K4BwqJ08U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "def get_dataloader(trainset, valset = None, batch_size = 64, num_workers = 2):\n",
        "    train_loader = DataLoader(trainset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\n",
        "    if valset:\n",
        "      val_loader = DataLoader(valset, shuffle=False, num_workers=num_workers, batch_size=batch_size)\n",
        "      return (train_loader, val_loader)\n",
        "    return (train_loader, None)"
      ],
      "metadata": {
        "id": "9Onw0XBfJ0Od"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemProfileDataset(Dataset):\n",
        "    def __init__(self,input):\n",
        "        \"\"\"\n",
        "        input: (num_items, 768) tensor\n",
        "        \"\"\"\n",
        "        self.vecs = input\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vecs.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        v = self.vecs[idx]\n",
        "        return v, v"
      ],
      "metadata": {
        "id": "nlDz-o5cLIYR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training DAE."
      ],
      "metadata": {
        "id": "cWLCVorq724q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_DAE_epoch(model, train_loader, optimizer, criterion, device):\n",
        "  \"\"\"Train autoencoder for one epoch\"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  for x, y in tqdm(train_loader, desc=\"Training DAE\"):\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      _, out = model.forward(x)\n",
        "      loss = criterion(out, y)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return total_loss / len(train_loader)\n",
        "\n",
        "def train_DAE(model, trainset, optimizer, device, n_epochs=10, batch_size=64, lr=0.001):\n",
        "    \"\"\"Train deep autoencoder\"\"\"\n",
        "    model.to(device)\n",
        "    train_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        avg_loss = train_DAE_epoch(model, train_loader,\n",
        "                                          optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "g7pI78Qj764b"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dae_model(dae_model, filepath='dae_model.pt'):\n",
        "    \"\"\"Save trained DAE model\"\"\"\n",
        "    torch.save(dae_model.state_dict(), filepath)\n",
        "    print(f\"DAE model saved to {filepath}\")\n",
        "\n",
        "def train_and_save_dae(item_profiles_dict, item_ids,\n",
        "                       hidden_dim=20, batch_size=64,\n",
        "                       n_epochs=10, device='cpu',\n",
        "                       save_path='dae_model.pt'):\n",
        "    \"\"\"\n",
        "    Train DAE on item profiles and save it\n",
        "\n",
        "    Args:\n",
        "        item_profiles_dict: dict mapping item_id to profile text\n",
        "        item_ids: array of unique item IDs\n",
        "        hidden_dim: output dimension of encoder\n",
        "        batch_size: training batch size\n",
        "        n_epochs: number of training epochs\n",
        "        device: 'cpu' or 'cuda'\n",
        "        save_path: path to save the trained model\n",
        "\n",
        "    Returns:\n",
        "        trained DAE model\n",
        "    \"\"\"\n",
        "    print(\"=== Training DAE Model ===\")\n",
        "\n",
        "    # Load BERT\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
        "                                          output_hidden_states=True)\n",
        "\n",
        "    # Prepare item texts\n",
        "    unique_items = np.unique(item_ids)\n",
        "    item_texts = []\n",
        "    for item_id in unique_items:\n",
        "        if item_id in item_profiles_dict:\n",
        "            item_texts.append(item_profiles_dict[item_id])\n",
        "        else:\n",
        "            item_texts.append(\"\")\n",
        "\n",
        "    # Generate BERT embeddings\n",
        "    print(\"Generating BERT embeddings...\")\n",
        "    bert_vecs = get_vector_batch(item_texts, bert_model, bert_tokenizer,\n",
        "                                 batch_size=batch_size, device=device)\n",
        "\n",
        "    # Train DAE\n",
        "    print(\"Training Deep Autoencoder...\")\n",
        "    dae_model = DeepAutoEncoder(input_dim=768, hidden_dim=hidden_dim)\n",
        "    dae_dataset = ItemProfileDataset(bert_vecs)\n",
        "    dae_model = train_DAE(dae_model, dae_dataset,\n",
        "                                  n_epochs=n_epochs,\n",
        "                                  batch_size=batch_size,\n",
        "                                  device=device)\n",
        "\n",
        "    # Save model\n",
        "    save_dae_model(dae_model, save_path)\n",
        "\n",
        "    return dae_model"
      ],
      "metadata": {
        "id": "ZfyQBC6mirj3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "def train_and_save_model(model, trainset, optimizer, device, batch_size=64, n_epochs=10, filepath=None):\n",
        "    model.to(device)\n",
        "    # train_loader, val_loader = get_dataloader(trainset, None, batch_size)\n",
        "    if filepath is None:\n",
        "        filepath = f\"models/trained_{model.__class__.__name__}.pt\"\n",
        "    model = train_DAE(model, trainset, optimizer, device, n_epochs, batch_size)\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    return model"
      ],
      "metadata": {
        "id": "i5KwNdagXlCF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using DAE to reduce dimensions."
      ],
      "metadata": {
        "id": "ZTl5G3v0YLCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###DEBUGGING: EXCLUDED FROM SCRIPT\n",
        "def get_reduced(model, input):\n",
        "  data = torch.FloatTensor(input).to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    reduced, _ = model.forward(data)\n",
        "  return reduced"
      ],
      "metadata": {
        "id": "HDyCeLqqYWNL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement factorization."
      ],
      "metadata": {
        "id": "ttYJHtT08B6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(X):\n",
        "  X = X.numpy()\n",
        "  numer = X @ X.T\n",
        "\n",
        "  sq = np.square(X)\n",
        "  sq_sum = sq.sum(axis=1)\n",
        "  sqrt_sum = np.sqrt(sq_sum)\n",
        "  denom = np.outer(sqrt_sum, sqrt_sum)\n",
        "\n",
        "  result = np.divide(numer, denom)\n",
        "  # print(result)\n",
        "  result = torch.from_numpy(result)\n",
        "  return result"
      ],
      "metadata": {
        "id": "-KzvgdcEzjgK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_factors=20, theta: torch.Tensor = None,  # [num_items, d]\n",
        "                  k = 3,\n",
        "                  beta= 0.01):\n",
        "        \"\"\"\n",
        "        theta: the iteem profiles acquired from BERT+DAE\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.f = num_factors\n",
        "        self.beta = beta\n",
        "        self.P = nn.Embedding(num_users, num_factors)\n",
        "        self.Q = nn.Embedding(num_items, num_factors)\n",
        "\n",
        "        #initialize randomly\n",
        "        nn.init.normal_(self.P.weight, std=0.01)\n",
        "        nn.init.normal_(self.Q.weight, std=0.01)\n",
        "\n",
        "        if theta is not None:\n",
        "            self.get_k_neighbors(theta, k)\n",
        "        else:\n",
        "            self.i_plus = None\n",
        "            self.i_sims = None\n",
        "\n",
        "        self.register_buffer(\"gamma\", torch.ones(num_items))\n",
        "\n",
        "    # get the k most similar items to each item\n",
        "    def get_k_neighbors(self, theta, k):\n",
        "        sim = cosine_similarity(theta)\n",
        "        sim.fill_diagonal_(0)\n",
        "        val, idx = torch.topk(sim, k=k, dim=1)\n",
        "        self.register_buffer(\"i_plus\", idx)\n",
        "        self.register_buffer(\"i_sims\", val)\n",
        "\n",
        "    def set_gamma(self, item_count, z=5.0):\n",
        "        count = item_count.float().clamp_min(1.0)\n",
        "        self.gamma = z / count\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        p = self.P(user_id)\n",
        "        q = self.Q(item_id)\n",
        "        return (p * q).sum(1)\n",
        "\n",
        "    # objective function\n",
        "    def loss(self, user_id, item_id, rating):\n",
        "        pred = self.forward(user_id, item_id)\n",
        "        error = 0.5 * (rating - pred).square().sum()\n",
        "\n",
        "        p = self.P(user_id)\n",
        "        q = self.Q(item_id)\n",
        "        reg = self.beta * (p.square().sum() + q.square().sum())\n",
        "\n",
        "        # this is the supplementary information regularization\n",
        "        if (self.i_plus is not None) and (self.i_sims is not None):\n",
        "            unique = torch.unique(item_id)\n",
        "            q_self = self.Q(unique)\n",
        "            q_neighbor = self.Q(self.i_plus[unique])\n",
        "            diff = (q_self.unsqueeze(1) - q_neighbor).square().sum(dim=2)\n",
        "            sim = self.i_sims[unique]\n",
        "            gamma = self.gamma[unique].unsqueeze(1)\n",
        "            reg += (diff * sim * gamma).sum()\n",
        "        return error + reg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMu1K0O58JEG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_MF_epoch(model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  for u, i, r in tqdm(train_loader):\n",
        "      u = u.to(device)\n",
        "      i = i.to(device)\n",
        "      r = r.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.loss(u, i, r)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(train_loader)\n",
        "\n",
        "def train_MF(model, n_epochs, train_loader, optimizer, verbose=False):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "      avg_loss = train_MF_epoch(model, train_loader, optimizer, device)\n",
        "      print(f\"Epoch {epoch:02d} | loss = {avg_loss:.4f}\")\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "yXBEy-aO4Yk1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct ALDRS class."
      ],
      "metadata": {
        "id": "a3AwxjkP8JgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RatingsDataset(Dataset):\n",
        "  def __init__(self, user_ids, item_ids, ratings):\n",
        "    self.user_ids = torch.as_tensor(user_ids, dtype=torch.long)\n",
        "    self.item_ids = torch.as_tensor(item_ids, dtype=torch.long)\n",
        "    self.ratings = torch.as_tensor(ratings,  dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ratings)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (\n",
        "      self.user_ids[idx],\n",
        "      self.item_ids[idx],\n",
        "      self.ratings[idx],\n",
        "    )"
      ],
      "metadata": {
        "id": "Eqi_G1Zf8L0z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_adlrs_pipeline(\n",
        "    user_ids,\n",
        "    item_ids,\n",
        "    ratings,\n",
        "    item_profiles_dict,\n",
        "    dae_file=None,\n",
        "    num_factors=20,\n",
        "    k_neighbors=3,\n",
        "    z_gamma=5.0,\n",
        "    batch_size=64,\n",
        "    dae_epochs=10,\n",
        "    mf_epochs=20,\n",
        "    lr=1e-2,\n",
        "    device='cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete ADLRS pipeline\n",
        "\n",
        "    Args:\n",
        "        user_ids: array of user IDs\n",
        "        item_ids: array of item IDs\n",
        "        ratings: array of ratings\n",
        "        item_profiles_dict: dict mapping item_id to profile text\n",
        "        ...\n",
        "\n",
        "    Returns:\n",
        "        trained model and item embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Map IDs to continuous indices\n",
        "    unique_users = np.unique(user_ids)\n",
        "    unique_items = np.unique(item_ids)\n",
        "\n",
        "    user2idx = {u: i for i, u in enumerate(unique_users)}\n",
        "    item2idx = {i: j for j, i in enumerate(unique_items)}\n",
        "\n",
        "    u_idx = np.array([user2idx[u] for u in user_ids])\n",
        "    i_idx = np.array([item2idx[i] for i in item_ids])\n",
        "\n",
        "    num_users = len(unique_users)\n",
        "    num_items = len(unique_items)\n",
        "\n",
        "    print(f\"Users: {num_users}, Items: {num_items}, Ratings: {len(ratings)}\")\n",
        "\n",
        "    # Step 1: Generate BERT embeddings\n",
        "    print(\"\\n=== Step 1: Generating BERT Embeddings ===\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
        "                                          output_hidden_states=True)\n",
        "\n",
        "    # Prepare item profiles in correct order\n",
        "    item_texts = []\n",
        "    for item_id in unique_items:\n",
        "        if item_id in item_profiles_dict:\n",
        "            item_texts.append(item_profiles_dict[item_id])\n",
        "        else:\n",
        "            item_texts.append(\"\")  # Empty profile for missing items\n",
        "\n",
        "    bert_vecs = get_vector_batch(item_texts, bert_model, bert_tokenizer,\n",
        "                                 batch_size=batch_size, device=device)\n",
        "\n",
        "    # Step 2: Load or Train Deep Autoencoder\n",
        "    if dae_file is not None:\n",
        "        print(f\"\\n=== Step 2: Loading Pre-trained DAE from {dae_file} ===\")\n",
        "        dae_model = DeepAutoEncoder(input_dim=768, hidden_dim=num_factors)\n",
        "        dae_model.load_state_dict(torch.load(dae_file, map_location=device))\n",
        "        dae_model.to(device)\n",
        "        print(\"DAE model loaded successfully\")\n",
        "    else:\n",
        "        print(\"\\n=== Step 2: Training Deep Autoencoder ===\")\n",
        "        dae_model = DeepAutoEncoder(input_dim=768, hidden_dim=num_factors)\n",
        "        dae_dataset = ItemProfileDataset(bert_vecs)\n",
        "        dae_model = train_DAE(dae_model, dae_dataset,\n",
        "                                      n_epochs=dae_epochs,\n",
        "                                      batch_size=batch_size,\n",
        "                                      device=device)\n",
        "\n",
        "    # Step 3: Get reduced item representations\n",
        "    print(\"\\n=== Step 3: Extracting Item Features ===\")\n",
        "    dae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        bert_vecs = bert_vecs.to(device)\n",
        "        reduced, _ = dae_model(bert_vecs)\n",
        "        reduced = reduced.cpu()\n",
        "\n",
        "    # Step 4: Build and train MF model\n",
        "    print(\"\\n=== Step 4: Training Matrix Factorization ===\")\n",
        "    model = MF(\n",
        "        num_users=num_users,\n",
        "        num_items=num_items,\n",
        "        num_factors=num_factors,\n",
        "        theta=reduced,\n",
        "        k=k_neighbors,\n",
        "        beta=0.01,\n",
        "    ).to(device)\n",
        "\n",
        "    # Set gamma based on item rating counts\n",
        "    item_counts = torch.bincount(\n",
        "        torch.as_tensor(i_idx, dtype=torch.long),\n",
        "        minlength=num_items,\n",
        "    )\n",
        "    model.set_gamma(item_counts, z=z_gamma)\n",
        "\n",
        "    # Prepare training data\n",
        "    train_dataset = RatingsDataset(u_idx, i_idx, ratings)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    trained_model = train_MF(model, mf_epochs, train_loader,\n",
        "                            optimizer, device, verbose=True)\n",
        "\n",
        "    return {\n",
        "        'model': trained_model,\n",
        "        'theta': reduced,\n",
        "        'user2idx': user2idx,\n",
        "        'item2idx': item2idx,\n",
        "        'dae_model': dae_model\n",
        "    }\n",
        "\n",
        "def get_rating_pred(model, user_id, item_id, user2idx, item2idx, device):\n",
        "    model.eval()\n",
        "    u_idx = torch.tensor([user2idx[u] for u in user_id], dtype=torch.long).to(device)\n",
        "    i_idx = torch.tensor([item2idx[i] for i in item_id], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred = model.forward(u_idx, i_idx)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "fQ2ViXB3Bk4t"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_adlrs_model(result, filepath='adlrs_model.pt', config=None):\n",
        "    \"\"\"\n",
        "    Save complete ADLRS model including MF model and metadata\n",
        "\n",
        "    Args:\n",
        "        result: dict returned from run_adlrs_pipeline\n",
        "        filepath: path to save the model\n",
        "        config: optional dict with configuration parameters\n",
        "    \"\"\"\n",
        "    save_dict = {\n",
        "        'mf_model_state_dict': result['model'].state_dict(),\n",
        "        'user2idx': result['user2idx'],\n",
        "        'item2idx': result['item2idx'],\n",
        "        'theta': result['theta'],\n",
        "        'num_users': result['model'].num_users,\n",
        "        'num_items': result['model'].num_items,\n",
        "        'num_factors': result['model'].f,\n",
        "    }\n",
        "\n",
        "    if config is not None:\n",
        "        save_dict['config'] = config\n",
        "\n",
        "    torch.save(save_dict, filepath)\n",
        "    print(f\"ADLRS model saved to {filepath}\")\n",
        "\n",
        "def load_adlrs_model(filepath, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load saved ADLRS model\n",
        "\n",
        "    Args:\n",
        "        filepath: path to saved model\n",
        "        device: 'cpu' or 'cuda'\n",
        "\n",
        "    Returns:\n",
        "        dict with model and metadata\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "\n",
        "    # Reconstruct MF model\n",
        "    model = MF(\n",
        "        num_users=checkpoint['num_users'],\n",
        "        num_items=checkpoint['num_items'],\n",
        "        num_factors=checkpoint['num_factors'],\n",
        "        theta=checkpoint['theta'],\n",
        "        k=checkpoint.get('config', {}).get('k_neighbors', 3),\n",
        "        beta=checkpoint.get('config', {}).get('beta', 0.01)\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(checkpoint['mf_model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"ADLRS model loaded from {filepath}\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'user2idx': checkpoint['user2idx'],\n",
        "        'item2idx': checkpoint['item2idx'],\n",
        "        'theta': checkpoint['theta'],\n",
        "        'config': checkpoint.get('config', {})\n",
        "    }"
      ],
      "metadata": {
        "id": "8c0-DjR8h7Ov"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement evaluation metrics."
      ],
      "metadata": {
        "id": "6yMcxEio8MWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MAE(y_true, y_pred):\n",
        "  return np.mean(np.abs(y_pred - y_true))\n",
        "\n",
        "def RMSE(y_true, y_pred):\n",
        "  return np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
        "\n",
        "def HR(scores, user_items, N):\n",
        "  \"\"\"\n",
        "  Compute Hit Ratio @ N for the whole dataset.\n",
        "\n",
        "  Args:\n",
        "      scores (dict): user_id -> {item_id: predicted_score}\n",
        "      user_items (dict): user_id -> set of ground-truth relevant items\n",
        "      N (int): cutoff N in top-N\n",
        "\n",
        "  Returns:\n",
        "      float: Hit Ratio @ N\n",
        "  \"\"\"\n",
        "  hits = 0\n",
        "  users = 0\n",
        "\n",
        "  for user, relevant_items in user_items.items():\n",
        "\n",
        "      # Skip users with no ground truth items\n",
        "      if len(relevant_items) == 0:\n",
        "          continue\n",
        "\n",
        "      users += 1\n",
        "      # sort scores for this user, descending\n",
        "      ranked_items = sorted(scores[user].items(),\n",
        "                            key=lambda x: x[1],\n",
        "                            reverse=True)\n",
        "\n",
        "      topN_items = [item for item, _ in ranked_items[:N]]\n",
        "\n",
        "      # check if *any* relevant item appears in top-N\n",
        "      if any(item in topN_items for item in relevant_items):\n",
        "          hits += 1\n",
        "\n",
        "  # avoid division by zero\n",
        "  if users == 0:\n",
        "      return 0.0\n",
        "  return hits / users\n"
      ],
      "metadata": {
        "id": "DQesBZlO8QJ5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training ADLRS model."
      ],
      "metadata": {
        "id": "gjf6M2EvWjfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================\n",
        "# Load and Prepare Data\n",
        "# ============================================\n",
        "\n",
        "# Load movie data\n",
        "movies_path = \"movies.dat\"\n",
        "ratings_path = \"ratings.dat\"\n",
        "\n",
        "filmsdict = create_films_dict(movies_path)\n",
        "ratingsmat = create_ratings_matrix(ratings_path)\n",
        "\n",
        "# Parse ratings\n",
        "user_ids, item_ids, ratings = parse_ratings(ratingsmat)\n",
        "\n",
        "# Prepare item profiles\n",
        "item_profiles = prepare_item_profiles(filmsdict)\n",
        "\n",
        "print(f\"Loaded {len(user_ids)} ratings\")\n",
        "print(f\"Loaded {len(item_profiles)} item profiles\")\n",
        "\n",
        "# ============================================\n",
        "# Train-Test Split\n",
        "# ============================================\n",
        "\n",
        "# Split data (80-20)\n",
        "indices = np.arange(len(ratings))\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_users = user_ids[train_idx]\n",
        "train_items = item_ids[train_idx]\n",
        "train_ratings = ratings[train_idx]\n",
        "\n",
        "test_users = user_ids[test_idx]\n",
        "test_items = item_ids[test_idx]\n",
        "test_ratings = ratings[test_idx]\n",
        "\n",
        "print(f\"Training samples: {len(train_ratings)}\")\n",
        "print(f\"Test samples: {len(test_ratings)}\")\n",
        "\n",
        "# ============================================\n",
        "# Option 1: Train DAE separately and save it\n",
        "# ============================================\n",
        "\n",
        "USE_PRETRAINED_DAE = False  # Set to True to use pre-trained DAE\n",
        "DAE_PATH = \"trained_dae.pt\"\n",
        "\n",
        "if not USE_PRETRAINED_DAE:\n",
        "    print(\"\\n=== Training and Saving DAE Model ===\")\n",
        "    dae_model = train_and_save_dae(\n",
        "        item_profiles_dict=item_profiles,\n",
        "        item_ids=item_ids,\n",
        "        hidden_dim=20,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        device=device,\n",
        "        save_path=DAE_PATH\n",
        "    )\n",
        "    print(f\"DAE model saved to {DAE_PATH}\")\n",
        "\n",
        "# ============================================\n",
        "# Option 2: Run ADLRS Pipeline (with or without pre-trained DAE)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Running ADLRS Pipeline ===\")\n",
        "\n",
        "# If USE_PRETRAINED_DAE is True, it will load the DAE from DAE_PATH\n",
        "# If False, it will train a new DAE\n",
        "result = run_adlrs_pipeline(\n",
        "    user_ids=train_users,\n",
        "    item_ids=train_items,\n",
        "    ratings=train_ratings,\n",
        "    item_profiles_dict=item_profiles,\n",
        "    dae_file=DAE_PATH if USE_PRETRAINED_DAE else None,  # Load pre-trained or train new\n",
        "    num_factors=20,\n",
        "    k_neighbors=3,\n",
        "    z_gamma=5.0,\n",
        "    batch_size=64,\n",
        "    dae_epochs=10,  # Only used if dae_file is None\n",
        "    mf_epochs=30,\n",
        "    lr=0.01,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model = result['model']\n",
        "user2idx = result['user2idx']\n",
        "item2idx = result['item2idx']\n",
        "\n",
        "# ============================================\n",
        "# Evaluate on Test Set\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Evaluating on Test Set ===\")\n",
        "\n",
        "# Filter test data to only include known users/items\n",
        "test_mask = np.array([\n",
        "    (u in user2idx and i in item2idx)\n",
        "    for u, i in zip(test_users, test_items)\n",
        "])\n",
        "\n",
        "filtered_test_users = test_users[test_mask]\n",
        "filtered_test_items = test_items[test_mask]\n",
        "filtered_test_ratings = test_ratings[test_mask]\n",
        "\n",
        "print(f\"Test samples after filtering: {len(filtered_test_ratings)}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = get_rating_pred(\n",
        "    model,\n",
        "    filtered_test_users,\n",
        "    filtered_test_items,\n",
        "    user2idx,\n",
        "    item2idx,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Calculate metrics\n",
        "mae = MAE(filtered_test_ratings, predictions)\n",
        "rmse = RMSE(filtered_test_ratings, predictions)\n",
        "\n",
        "print(f\"\\nTest MAE: {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# Evaluate Cold Items (items with < 5 ratings)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Evaluating Cold Items ===\")\n",
        "\n",
        "# Count ratings per item in training set\n",
        "from collections import Counter\n",
        "train_item_counts = Counter(train_items)\n",
        "\n",
        "# Find cold items in test set\n",
        "cold_item_mask = np.array([\n",
        "    train_item_counts[i] < 5\n",
        "    for i in filtered_test_items\n",
        "])\n",
        "\n",
        "if cold_item_mask.sum() > 0:\n",
        "    cold_predictions = predictions[cold_item_mask]\n",
        "    cold_ratings = filtered_test_ratings[cold_item_mask]\n",
        "\n",
        "    cold_mae = MAE(cold_ratings, cold_predictions)\n",
        "    cold_rmse = RMSE(cold_ratings, cold_predictions)\n",
        "\n",
        "    print(f\"Cold items count: {cold_item_mask.sum()}\")\n",
        "    print(f\"Cold items MAE: {cold_mae:.4f}\")\n",
        "    print(f\"Cold items RMSE: {cold_rmse:.4f}\")\n",
        "else:\n",
        "    print(\"No cold items in test set\")\n",
        "\n",
        "# ============================================\n",
        "# Top-N Recommendation Example\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Top-N Recommendation Example ===\")\n",
        "\n",
        "# Get a sample user\n",
        "sample_user = filtered_test_users[0]\n",
        "print(f\"Sample user ID: {sample_user}\")\n",
        "\n",
        "# Get all items\n",
        "all_items = np.array(list(item2idx.keys()))\n",
        "\n",
        "# Predict ratings for all items\n",
        "sample_user_array = np.full(len(all_items), sample_user)\n",
        "all_predictions = get_rating_pred(\n",
        "    model,\n",
        "    sample_user_array,\n",
        "    all_items,\n",
        "    user2idx,\n",
        "    item2idx,\n",
        "    device\n",
        ")\n",
        "\n",
        "# Get top 10 recommendations\n",
        "top_k = 10\n",
        "top_indices = np.argsort(all_predictions)[-top_k:][::-1]\n",
        "top_items = all_items[top_indices]\n",
        "top_scores = all_predictions[top_indices]\n",
        "\n",
        "print(f\"\\nTop {top_k} recommendations:\")\n",
        "for rank, (item_id, score) in enumerate(zip(top_items, top_scores), 1):\n",
        "    item_info = filmsdict.get(item_id, f\"Item {item_id}\")\n",
        "    print(f\"{rank}. {item_info.split('::')[1]} (score: {score:.2f})\")\n",
        "\n",
        "# ============================================\n",
        "# Save Model\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Saving ADLRS Model ===\")\n",
        "\n",
        "# Save the complete model with configuration\n",
        "save_adlrs_model(\n",
        "    result=result,\n",
        "    filepath='adlrs_complete_model.pt',\n",
        "    config={\n",
        "        'num_factors': 20,\n",
        "        'k_neighbors': 3,\n",
        "        'z_gamma': 5.0,\n",
        "        'beta': 0.01,\n",
        "        'mf_epochs': 30,\n",
        "        'lr': 0.01\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# Load Model (for inference)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Loading Saved Model ===\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_result = load_adlrs_model('adlrs_complete_model.pt', device=device)\n",
        "\n",
        "loaded_model = loaded_result['model']\n",
        "loaded_user2idx = loaded_result['user2idx']\n",
        "loaded_item2idx = loaded_result['item2idx']\n",
        "\n",
        "print(f\"Loaded model configuration: {loaded_result['config']}\")\n",
        "\n",
        "# Use loaded model for predictions\n",
        "print(\"\\n=== Testing Loaded Model ===\")\n",
        "\n",
        "# Make predictions with loaded model\n",
        "loaded_predictions = get_rating_pred(\n",
        "    loaded_model,\n",
        "    filtered_test_users[:100],  # Test on first 100 samples\n",
        "    filtered_test_items[:100],\n",
        "    loaded_user2idx,\n",
        "    loaded_item2idx,\n",
        "    device\n",
        ")\n",
        "\n",
        "loaded_mae = MAE(filtered_test_ratings[:100], loaded_predictions)\n",
        "print(f\"Loaded model MAE (100 samples): {loaded_mae:.4f}\")\n",
        "print(\"Loaded model works correctly!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "z-0R_FAjWlwL",
        "outputId": "85bc6118-f7b6-42af-8c59-08434a779e8a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded 1000209 ratings\n",
            "Loaded 3883 item profiles\n",
            "Training samples: 800167\n",
            "Test samples: 200042\n",
            "\n",
            "=== Training and Saving DAE Model ===\n",
            "=== Training DAE Model ===\n",
            "Generating BERT embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating BERT embeddings:  52%|█████▏    | 30/58 [00:02<00:02, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2629180659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUSE_PRETRAINED_DAE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Training and Saving DAE Model ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     dae_model = train_and_save_dae(\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mitem_profiles_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem_profiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mitem_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3671509745.py\u001b[0m in \u001b[0;36mtrain_and_save_dae\u001b[0;34m(item_profiles_dict, item_ids, hidden_dim, batch_size, n_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Generate BERT embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating BERT embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     bert_vecs = get_vector_batch(item_texts, bert_model, bert_tokenizer, \n\u001b[0m\u001b[1;32m     44\u001b[0m                                  batch_size=batch_size, device=device)\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-563337830.py\u001b[0m in \u001b[0;36mget_vector_batch\u001b[0;34m(texts, bert_model, bert_tokenizer, batch_size, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    651\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 558\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 488\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtgt_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading ADLRS and making predictions."
      ],
      "metadata": {
        "id": "HzmgOVG0j6eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ADLRS Inference Script\n",
        "Load a saved ADLRS model and make predictions\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================\n",
        "# Load Saved Model\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Loading ADLRS Model ===\")\n",
        "\n",
        "model_path = 'adlrs_complete_model.pt'\n",
        "result = load_adlrs_model(model_path, device=device)\n",
        "\n",
        "model = result['model']\n",
        "user2idx = result['user2idx']\n",
        "item2idx = result['item2idx']\n",
        "config = result['config']\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Configuration: {config}\")\n",
        "print(f\"Number of users: {model.num_users}\")\n",
        "print(f\"Number of items: {model.num_items}\")\n",
        "print(f\"Latent factors: {model.f}\")\n",
        "\n",
        "# ============================================\n",
        "# Make Predictions for Specific Users\n",
        "# ============================================\n",
        "\n",
        "def recommend_for_user(model, user_id, user2idx, item2idx,\n",
        "                       top_k=10, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate top-K recommendations for a specific user\n",
        "\n",
        "    Args:\n",
        "        model: trained MF model\n",
        "        user_id: original user ID\n",
        "        user2idx: user ID to index mapping\n",
        "        item2idx: item ID to index mapping\n",
        "        top_k: number of recommendations\n",
        "        device: 'cpu' or 'cuda'\n",
        "\n",
        "    Returns:\n",
        "        list of (item_id, predicted_rating) tuples\n",
        "    \"\"\"\n",
        "    if user_id not in user2idx:\n",
        "        print(f\"User {user_id} not found in training data\")\n",
        "        return []\n",
        "\n",
        "    # Get all items\n",
        "    all_items = np.array(list(item2idx.keys()))\n",
        "\n",
        "    # Predict ratings for all items\n",
        "    user_array = np.full(len(all_items), user_id)\n",
        "    predictions = get_rating_pred(\n",
        "        model, user_array, all_items,\n",
        "        user2idx, item2idx, device\n",
        "    )\n",
        "\n",
        "    # Get top-K items\n",
        "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
        "    top_items = all_items[top_indices]\n",
        "    top_scores = predictions[top_indices]\n",
        "\n",
        "    recommendations = list(zip(top_items, top_scores))\n",
        "    return recommendations\n",
        "\n",
        "# ============================================\n",
        "# Example 1: Recommend for a single user\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Example 1: Recommendations for Single User ===\")\n",
        "\n",
        "# Get first user from user2idx\n",
        "sample_user_id = list(user2idx.keys())[0]\n",
        "print(f\"Generating recommendations for user {sample_user_id}\")\n",
        "\n",
        "recommendations = recommend_for_user(\n",
        "    model=model,\n",
        "    user_id=sample_user_id,\n",
        "    user2idx=user2idx,\n",
        "    item2idx=item2idx,\n",
        "    top_k=10,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nTop 10 recommendations:\")\n",
        "for rank, (item_id, score) in enumerate(recommendations, 1):\n",
        "    print(f\"{rank}. Item {item_id}: predicted rating = {score:.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# Example 2: Predict specific user-item pairs\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Example 2: Predict Specific Ratings ===\")\n",
        "\n",
        "# Create some user-item pairs to predict\n",
        "test_users = list(user2idx.keys())[:5]\n",
        "test_items = list(item2idx.keys())[:5]\n",
        "\n",
        "print(\"Predicting ratings for 5 user-item pairs:\")\n",
        "predictions = get_rating_pred(\n",
        "    model,\n",
        "    np.array(test_users),\n",
        "    np.array(test_items),\n",
        "    user2idx,\n",
        "    item2idx,\n",
        "    device\n",
        ")\n",
        "\n",
        "for user, item, pred in zip(test_users, test_items, predictions):\n",
        "    print(f\"User {user} - Item {item}: predicted rating = {pred:.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# Example 3: Batch predictions for multiple users\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Example 3: Batch Recommendations ===\")\n",
        "\n",
        "def batch_recommend(model, user_ids, user2idx, item2idx,\n",
        "                   top_k=5, device='cpu'):\n",
        "    \"\"\"Generate recommendations for multiple users\"\"\"\n",
        "    all_recommendations = {}\n",
        "\n",
        "    for user_id in user_ids:\n",
        "        recs = recommend_for_user(\n",
        "            model, user_id, user2idx, item2idx,\n",
        "            top_k, device\n",
        "        )\n",
        "        all_recommendations[user_id] = recs\n",
        "\n",
        "    return all_recommendations\n",
        "\n",
        "# Get recommendations for first 3 users\n",
        "batch_users = list(user2idx.keys())[:3]\n",
        "batch_recs = batch_recommend(\n",
        "    model, batch_users, user2idx, item2idx,\n",
        "    top_k=5, device=device\n",
        ")\n",
        "\n",
        "for user_id, recs in batch_recs.items():\n",
        "    print(f\"\\nUser {user_id} top 5 items:\")\n",
        "    for rank, (item_id, score) in enumerate(recs, 1):\n",
        "        print(f\"  {rank}. Item {item_id}: {score:.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# Example 4: Cold-start prediction for new user\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Example 4: Cold-Start Handling ===\")\n",
        "\n",
        "# Try to get recommendations for a user not in training set\n",
        "new_user_id = max(user2idx.keys()) + 1000\n",
        "print(f\"Attempting recommendations for new user {new_user_id}\")\n",
        "\n",
        "new_user_recs = recommend_for_user(\n",
        "    model, new_user_id, user2idx, item2idx,\n",
        "    top_k=5, device=device\n",
        ")\n",
        "\n",
        "if not new_user_recs:\n",
        "    print(\"Note: Model cannot make predictions for completely new users\")\n",
        "    print(\"For new users, you would need to:\")\n",
        "    print(\"  1. Collect some initial ratings\")\n",
        "    print(\"  2. Use item popularity or content-based recommendations\")\n",
        "    print(\"  3. Retrain the model to include the new user\")\n",
        "\n",
        "# ============================================\n",
        "# Example 5: Get similar items using theta\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Example 5: Find Similar Items ===\")\n",
        "\n",
        "def find_similar_items(item_id, theta, item2idx, top_k=5):\n",
        "    \"\"\"\n",
        "    Find similar items based on item embeddings\n",
        "\n",
        "    Args:\n",
        "        item_id: target item ID\n",
        "        theta: item embedding matrix\n",
        "        item2idx: item ID to index mapping\n",
        "        top_k: number of similar items to return\n",
        "\n",
        "    Returns:\n",
        "        list of (item_id, similarity) tuples\n",
        "    \"\"\"\n",
        "    if item_id not in item2idx:\n",
        "        print(f\"Item {item_id} not found\")\n",
        "        return []\n",
        "\n",
        "    idx = item2idx[item_id]\n",
        "    item_vec = theta[idx].unsqueeze(0)\n",
        "\n",
        "    # Compute cosine similarity with all items\n",
        "    sim = torch.nn.functional.cosine_similarity(\n",
        "        item_vec, theta, dim=1\n",
        "    )\n",
        "\n",
        "    # Get top-K (excluding the item itself)\n",
        "    sim[idx] = -1  # Exclude self\n",
        "    top_indices = torch.argsort(sim, descending=True)[:top_k]\n",
        "\n",
        "    # Map back to item IDs\n",
        "    idx2item = {v: k for k, v in item2idx.items()}\n",
        "    similar_items = [\n",
        "        (idx2item[i.item()], sim[i].item())\n",
        "        for i in top_indices\n",
        "    ]\n",
        "\n",
        "    return similar_items\n",
        "\n",
        "# Find similar items for a sample item\n",
        "sample_item_id = list(item2idx.keys())[0]\n",
        "print(f\"Finding items similar to item {sample_item_id}\")\n",
        "\n",
        "similar = find_similar_items(\n",
        "    item_id=sample_item_id,\n",
        "    theta=result['theta'],\n",
        "    item2idx=item2idx,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(f\"\\nTop 5 similar items:\")\n",
        "for rank, (item_id, similarity) in enumerate(similar, 1):\n",
        "    print(f\"{rank}. Item {item_id}: similarity = {similarity:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# Save predictions to file (optional)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n=== Saving Predictions ===\")\n",
        "\n",
        "# Generate predictions for all test users\n",
        "all_test_users = list(user2idx.keys())[:100]  # First 100 users\n",
        "all_predictions = {}\n",
        "\n",
        "for user_id in all_test_users:\n",
        "    recs = recommend_for_user(\n",
        "        model, user_id, user2idx, item2idx,\n",
        "        top_k=20, device=device\n",
        "    )\n",
        "    all_predictions[user_id] = recs\n",
        "\n",
        "# Save to numpy file\n",
        "np.save('predictions.npy', all_predictions)\n",
        "print(\"Predictions saved to predictions.npy\")\n",
        "\n",
        "print(\"\\n=== Inference Complete ===\")"
      ],
      "metadata": {
        "id": "fEVQdad8jx6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}